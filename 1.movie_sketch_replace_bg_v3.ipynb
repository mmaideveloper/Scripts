{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739f1de",
   "metadata": {},
   "source": [
    "Use YOLO to detect persons. More accurate as library solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ef5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "class SketchDanceEditor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_path,\n",
    "            output_path,\n",
    "            background_mode=\"color\",      # \"color\", \"image\", \"none\"\n",
    "            background_color=(210, 210, 210),\n",
    "            background_image=None,\n",
    "            clipsec=None,\n",
    "            show_personEdge=False,\n",
    "            sketch_persons=True,\n",
    "            face_mode = \"none\" , # \"image\",\n",
    "            face_image=None\n",
    "        ):\n",
    "\n",
    "        self.face_mode = face_mode\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.background_mode = background_mode\n",
    "        self.background_color = background_color\n",
    "        self.background_image = background_image\n",
    "        self.clipsec = clipsec\n",
    "        self.show_personEdge = show_personEdge\n",
    "        self.sketch_persons = sketch_persons\n",
    "\n",
    "        # Load YOLO segmentation model\n",
    "        # Small and fast: yolov8s-seg.pt\n",
    "        # If you want even better accuracy, replace with yolov8x-seg.pt\n",
    "        #self.detector = YOLO(\"yolov8s-seg.pt\")\n",
    "        self.detector = YOLO(\"yolov8x-seg.pt\")\n",
    "        \n",
    "        #face recognition\n",
    "        self.cartoon_face = face_image # Your cartoon face image\n",
    "        self.face_detector = mp.solutions.face_detection.FaceDetection(\n",
    "            model_selection=1,\n",
    "            min_detection_confidence=0.6\n",
    ")\n",
    "\n",
    "    def swap_face_with_cartoon(self, frame):\n",
    "        # Convert frame to RGB\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = self.face_detector.process(rgb)\n",
    "        if not results or not results.detections:\n",
    "            return frame\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        for detection in results.detections:\n",
    "            # Extract bounding box (normalized)\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x1 = int(box.xmin * w)\n",
    "            y1 = int(box.ymin * h)\n",
    "            box_w = int(box.width * w)\n",
    "            box_h = int(box.height * h)\n",
    "\n",
    "            # Boundary safety\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(w, x1 + box_w)\n",
    "            y2 = min(h, y1 + box_h)\n",
    "\n",
    "            # Resize cartoon face\n",
    "            resized_cartoon = cv2.resize(\n",
    "                self.cartoon_face,\n",
    "                (x2 - x1, y2 - y1),\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "\n",
    "            # Replace region\n",
    "            #frame[y1:y2, x1:x2] = resized_cartoon\n",
    "            alpha = 0.9\n",
    "            frame[y1:y2, x1:x2] = cv2.addWeighted(\n",
    "                frame[y1:y2, x1:x2],\n",
    "                1 - alpha,\n",
    "                resized_cartoon,\n",
    "                alpha,\n",
    "                0\n",
    "            )\n",
    "\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def detect_person_mask(self, frame):\n",
    "        # Convert BGR -> RGB\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = self.detector.predict(rgb, verbose=False)\n",
    "\n",
    "        # Start with empty binary mask\n",
    "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        if not hasattr(results[0], \"masks\") or results[0].masks is None:\n",
    "            return mask\n",
    "\n",
    "        for m, cls in zip(results[0].masks.data, results[0].boxes.cls):\n",
    "            if int(cls) == 0:  # 0 = 'person' class\n",
    "                person_mask = m.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                # Resize to full frame size\n",
    "                person_mask = cv2.resize(\n",
    "                    person_mask,\n",
    "                    (frame.shape[1], frame.shape[0]),\n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "\n",
    "                mask = np.bitwise_or(mask, person_mask)\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def sketch_frame(self, frame, mask=None):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        inv = 255 - gray\n",
    "        blur = cv2.GaussianBlur(inv, (21, 21), 0)\n",
    "        sketch = cv2.divide(gray, 255 - blur, scale=256)\n",
    "        return cv2.cvtColor(sketch, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    def get_background(self, frame, mask):\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        if self.background_mode == \"none\":\n",
    "            return frame.copy()\n",
    "\n",
    "        elif self.background_mode == \"color\":\n",
    "            return np.full((h, w, 3), self.background_color, dtype=np.uint8)\n",
    "\n",
    "        elif self.background_mode == \"image\":\n",
    "            if self.background_image is None:\n",
    "                raise ValueError(\"background_mode='image' but no background_image provided\")\n",
    "            return cv2.resize(self.background_image, (w, h))\n",
    "\n",
    "        raise ValueError(f\"Unknown background_mode: {self.background_mode}\")\n",
    "\n",
    "    def process_video(self):\n",
    "        clip = VideoFileClip(self.input_path).subclip(*self.clipsec) if self.clipsec else VideoFileClip(self.input_path)\n",
    "        processed_frames = []\n",
    "\n",
    "        for frame in clip.iter_frames():\n",
    "            mask = self.detect_person_mask(frame)\n",
    "\n",
    "            background = self.get_background(frame,mask)\n",
    "                     \n",
    "            foreground = np.where(mask[..., None] == 1, frame, background)\n",
    "\n",
    "            if self.sketch_persons:\n",
    "                final = self.sketch_frame(foreground, mask)\n",
    "            else:\n",
    "                final = foreground\n",
    "\n",
    "            if self.show_personEdge:\n",
    "                edges = cv2.Canny((mask * 255).astype(np.uint8), 80, 180)\n",
    "                final[edges != 0] = [255, 0, 0]\n",
    "                \n",
    "            if self.face_mode == \"image\":\n",
    "                face = self.swap_face_with_cartoon(final)\n",
    "            else:\n",
    "                face = final\n",
    "             \n",
    "\n",
    "            processed_frames.append(face)\n",
    "\n",
    "        out_clip = ImageSequenceClip(processed_frames, fps=clip.fps)\n",
    "        out_clip.write_videofile(self.output_path, codec=\"libx264\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c6992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Image loaded successfully.\n",
      "Moviepy - Building video output_tango1_v19_ocean.mp4.\n",
      "Moviepy - Writing video output_tango1_v19_ocean.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_tango1_v19_ocean.mp4\n"
     ]
    }
   ],
   "source": [
    "#setup prompt and background image\n",
    "prompt_face = \"\"\"\n",
    "Replace face with a generic, unrecognizable synthetic face of a young adult male/female with neutral expression\n",
    "\"\"\"\n",
    "\n",
    "bg = cv2.imread(\"./backstage/ballroom.png\")  # or \"dancefloor.jpg\"\n",
    "\n",
    "if bg is None:\n",
    "    print(\"❌ Image not found or failed to load.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"✅ Image loaded successfully.\")\n",
    "\n",
    "\n",
    "img = np.full((512, 512, 3), 220, dtype=np.uint8)\n",
    "pil_bg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "editor = SketchDanceEditor(\n",
    "    input_path=\"./solus/tango1.mp4\",\n",
    "    output_path=\"output_tango1_v20_ballroom.mp4\",\n",
    "    clipsec=(0,15),\n",
    "    background_mode=\"image\",      # \"color\", \"image\", \"none\"\n",
    "    background_color=(210, 210, 210),\n",
    "    background_image= bg,\n",
    "    show_personEdge=False,\n",
    "    sketch_persons=True,\n",
    "    face_mode = \"none\",\n",
    "    face_image = None\n",
    ")\n",
    "editor.process_video()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimovie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
